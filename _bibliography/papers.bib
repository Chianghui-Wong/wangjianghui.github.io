---
---

@string{cvpr = {Proceedings of the IEEE conference on computer vision and pattern recognition}}

@inproceedings{xie2021cooperative,
  title={Cooperative Training of Fast Thinking Initializer and Slow Thinking Solver for Multi-Modal Conditional Learning },
  author={Xie, Jianwen and Zheng, Zilong and Fang, Xiaolin and Zhu, Song-Chun and Wu, Ying Nian},
  booktitle={IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},
  abbr={TPAMI},
  year={2021},
  abstract={This paper studies the supervised learning of the conditional distribution of a high-dimensional output given an input, where the output and input may belong to two different modalities, e.g., the output is an photo image and the input is a sketch image. We solve this problem by cooperative training of a fast thinking initializer and slow thinking solver. The initializer generates the output directly by a non-linear transformation of the input as well as a noise vector that accounts for latent variability in the output. The slow thinking solver learns an objective function in the form of a conditional energy function, so that the output can be generated by optimizing the objective function, or more rigorously by sampling from the conditional energy-based model. We propose to learn the two models jointly, where the fast thinking initializer serves to initialize the sampling of the slow thinking solver, and the solver refines the initial output by an iterative algorithm. The solver learns from the difference between the refined output and the observed output, while the initializer learns from how the solver refines its initial output. We demonstrate the effectiveness of the proposed method on various multi-modal conditional learning tasks, e.g., class-to-image generation, image-to-image translation, and image recovery.},
  arXiv={https://arxiv.org/pdf/1902.02812.pdf}
}

@inproceedings{zheng2021PatchGConvNet,
    title={Patchwise Generative ConvNet: Training an Energy-Based Model from a Single Natural Image for Internal Learning},
    author={Zheng, Zilong and Xie, Jianwen and Li, Ping},
    booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)},
    year={2021},
    abbr={CVPR'21},
    img={SinEBM/pipeline.png},
} 

@inproceedings{fan2021learning,
    title={Learning Triadic Belief Dynamics in Nonverbal Communication from Videos},
    author={Fan, Lifeng and Qiu, Shuwen and Zheng, Zilong and Gao, Tao and Zhu, Song-Chun and Zhu, Yixin},
    booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)},
    year={2021},
    abbr={CVPR'21},
    award={Oral},
    img={TriadicBelief/pg.png}

} 

@article{xie2021GPointent,
    title={Generative PointNet: Deep Energy-Based Learning on Unordered Point Sets for 3D Generation, Reconstruction and Classification},
    author={Xie, Jianwen and Xu, Yifei and Zheng, Zilong and Zhu, Song-Chun and Wu, Ying Nian},
    journal={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    year={2021},
    abbr={CVPR'21},
    img={GPointNet/gen_res.png},
    pdf={https://arxiv.org/pdf/2004.01301.pdf},
    abstract={We propose a generative model of unordered point sets, such as point clouds, in the forms of an energy-based model, where the energy function is parameterized by an input-permutation-invariant bottom-up neural network. The energy function learns a coordinate encoding of each point and then aggregates all individual point features into energy for the whole point cloud. We show that our model can be derived from the discriminative PointNet. The model can be trained by MCMC-based maximum likelihood learning (as well as its variants), without the help of any assisting networks like those in GANs and VAEs. Unlike most point cloud generator that relys on hand-crafting distance metrics, our model does not rely on hand-crafting distance metric for point cloud generation, because it synthesizes point clouds by matching observed examples in terms of statistical property defined by the energy function. Furthermore, we can learn a short-run MCMC toward the energy-based model as a flow-like generator for point cloud reconstruction and interpretation. The learned point cloud representation can be also useful for point cloud classification. Experiments demonstrate the advantages of the proposed generative model of point clouds.},
} 

@article{xie2021cycle,
    title={Learning Cycle-Consistent Cooperative Networks via Alternating MCMC Teaching for Unsupervised Cross-Domain Translation},
    author={Xie, Jianwen and Zheng, Zilong and Fang, Xiaolin and Zhu, Song-Chun and Wu, Ying Nian},
    journal={The Thirty-Third AAAI Conference on Artificial Intelligence (AAAI)},
    year={2021},
    abbr={AAAI'21},
    img={CycleCoop/style_transfer.png},
    pdf={CycleCoop/AAAI2021_CycleCoopNets.pdf},
    abstract={This paper studies the unsupervised cross-domain translation problem by proposing a generative framework, in which the probability distribution of each domain is represented by a generative cooperative network that consists of an energy-based model and a latent variable model. The use of generative cooperative network enables maximum likelihood learning of the domain model by MCMC teaching, where the energy-based model seeks to fit the data distribution of domain and distills its knowledge to the latent variable model via MCMC. Specifically, in the MCMC teaching process, the latent variable model parameterized by an encoder-decoder maps examples from the source domain to the target domain, while the energy-based model further refines the mapped results by Langevin revision such that the revised results match to the examples in the target domain in terms of the statistical properties, which are defined by the learned energy function. For the purpose of building up a correspondence between two unpaired domains, the proposed framework simultaneously learns a pair of cooperative networks with cycle consistency, accounting for a two-way translation between two domains, by alternating MCMC teaching. Experiments show that the proposed framework is useful for unsupervised image-to-image translation and unpaired image sequence translation.},
    website={http://www.stat.ucla.edu/&sim;jxie/CycleCoopNets/}

} 

@article{xie2021vaeebm,
    title={Energy-Based Probability Estimation with Variational Ancestral Langevin Sampler},
    author={Xie, Jianwen and Zheng, Zilong and Li, Ping},
    journal={The Thirty-Third AAAI Conference on Artificial Intelligence (AAAI)},
    year={2021},
    abbr={AAAI'21},
    img={VaeEBM/mcmc_teaching.jpg}
} 


@inproceedings{yuan2020joint,
    title={Joint Inference of States, Robot Knowledge, and Human (False-)Beliefs},
    author={Yuan, Tao and Liu, Hangxin and Fan, Lifeng and Zheng, Zilong and Gao, Tao and Zhu, Yixin and Zhu, Song-Chun},
    booktitle={Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},
    year={2020},
    abbr={ICRA'20},
    img={TomICRA20/tom_icra20.png},
    pdf={TomICRA20/icra20_tom.pdf},
    abstract={Aiming to understand how human (false-)belief—a core socio-cognitive ability—would affect human interactions with robots, this paper proposes to adopt a graphical model to unify the representation of object states, robot knowledge, and human (false-)beliefs. Specifically, a parse graph (PG) is learned from a single-view spatiotemporal parsing by aggregating various object states along the time; such a learned representation is accumulated as the robot’s knowledge. An inference algorithm is derived to fuse individual PG from all robots across multi-views into a joint PG, which affords more effective reasoning and inference capability to overcome the errors originated from a single view. In the experiments, through the joint inference over PGs, the system correctly recognizes human (false-)belief in various settings and achieves better cross-view accuracy on a challenging small object tracking dataset.},
}

@inproceedings{xie2020motion,
    title={Motion-Based Generator Model: Unsupervised Disentanglement of Appearance, Trackable and Intrackable Motions in Dynamic Patterns},
    author={Xie, Jianwen and Gao, Ruiqi and Zheng, Zilong and Zhu, Song-Chun and Wu, Ying Nian},
    booktitle={The Thirty-Third AAAI Conference on Artificial Intelligence (AAAI)},
    year={2020},
    abbr={AAAI'20},
    img={Motion-DynGen/deform.gif},
    pdf={https://arxiv.org/pdf/1911.11294.pdf},
    award={Oral},
    abstract={Dynamic patterns are characterized by complex spatial and motion patterns. Understanding dynamic patterns requires a disentangled representational model that separates the factorial components. A commonly used model for dynamic patterns is the state space model, where the state evolves over time according to a transition model and the state generates the observed image frames according to an emission model. To model the motions explicitly, it is natural for the model to be based on the motions or the displacement fields of the pixels. Thus in the emission model, we let the hidden state generate the displacement field, which warps the trackable component in the previous image frame to generate the next frame while adding a simultaneously emitted residual image to account for the change that cannot be explained by the deformation. The warping of the previous image is about the trackable part of the change of image frame, while the residual image is about the intrackable part of the image. We use a maximum likelihood algorithm to learn the model parameters that iterates between inferring latent noise vectors that drive the transition model and updating the parameters given the inferred latent vectors. Meanwhile we adopt a regularization term to penalize the norms of the residual images to encourage the model to explain the change of image frames by trackable motion. Unlike existing methods on dynamic patterns, we learn our model in unsupervised setting without ground truth displacement fields or optical flows. In addition, our model defines a notion of intrackability by the separation of warped component and residual component in each image frame. We show that our method can synthesize realistic dynamic pattern, and disentangling appearance, trackable and intrackable motions. The learned models can be useful for motion transfer, and it is natural to adopt it to define and measure intrackability of a dynamic pattern.},
    website={http://www.stat.ucla.edu/~jxie/MotionBasedGenerator/MotionBasedGenerator.html}
} 

@inproceedings{zheng2019reasoning,
    title={Reasoning Visual Dialogs with Structural and Partial Observations},
    author={Zheng, Zilong and Wang, Wenguan and Qi, Siyuan and Zhu, Song-Chun},
    booktitle={Computer Vision and Pattern Recognition (CVPR), 2019 IEEE Conference on},
    year={2019},
    abbr={CVPR'19},
    img={VisDial-GNN/visdial.jpg},
    pdf={https://arxiv.org/pdf/1904.05548.pdf},
    code={https://github.com/zilongzheng/visdial-gnn},
    award={Oral},
    abstract={We propose a novel model to address the task of Visual Dialog which exhibits complex dialog structures. To obtain a reasonable answer based on the current question and the dialog history, the underlying semantic dependencies between dialog entities are essential. In this paper, we explicitly formalize this task as inference in a graphical model with partially observed nodes and unknown graph structures (relations in dialog). The given dialog entities are viewed as the observed nodes. The answer to a given question is represented by a node with missing value. We first introduce an Expectation Maximization algorithm to infer both the underlying dialog structures and the missing node values (desired answers). Based on this, we proceed to propose a differentiable graph neural network (GNN) solution that approximates this process. Experiment results on the VisDial and VisDial-Q datasets show that our model outperforms comparative methods. It is also observed that our method can infer the underlying dialog structure for better dialog reasoning.},
} 

@article{xie2019DG,
    title = {Learning Dynamic Generator Model by Alternating Back-Propagation Through Time},
    author = {Xie, Jianwen and Gao, Ruiqi and Zheng, Zilong and Zhu, Song-Chun and Wu, Ying Nian},
    journal={The Thirty-Third AAAI Conference on Artificial Intelligence (AAAI)},
    year = {2019},
    abbr={AAAI'19},
    img={DynamicGenerator/dynamic_generator.gif},
    pdf={https://arxiv.org/pdf/1812.10587.pdf},
    code={https://github.com/jianwen-xie/Dynamic_generator},
    award={Spotlight},
    abstract={This paper studies the dynamic generator model for spatial-temporal processes such as dynamic textures and action sequences in video data. In this model, each time frame of the video sequence is generated by a generator model, which is a non-linear transformation of a latent state vector, where the non-linear transformation is parametrized by a top-down neural network. The sequence of latent state vectors follows a non-linear auto-regressive model, where the state vector of the next frame is a non-linear transformation of the state vector of the current frame as well as an independent noise vector that provides randomness in the transition. The non-linear transformation of this transition model can be parametrized by a feedforward neural network. We show that this model can be learned by an alternating back-propagation through time algorithm that iteratively samples the noise vectors and updates the parameters in the transition model and the generator model. We show that our training method can learn realistic models for dynamic textures and action patterns.},
    website={http://www.stat.ucla.edu/~jxie/DynamicGenerator/DynamicGenerator.html}
}    

@inproceedings{xie2018learning,
  title={Learning Descriptor Networks for 3D Shape Synthesis and Analysis},
  author={Xie, Jianwen and Zheng, Zilong and Gao, Ruiqi and Wang, Wenguan and Zhu, Song-Chun and Wu, Ying Nian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)},
  pages={8629--8638},
  year={2018},
  abbr={CVPR'18},
  img={3DDescriptorNet/3ddescriptor.png},
  pdf={https://arxiv.org/pdf/1804.00586.pdf},
  award={Oral},
  code={https://github.com/jianwen-xie/3DDescriptorNet},
  abstract={This paper proposes a 3D shape descriptor network, which is a deep convolutional energy-based model, for modeling volumetric shape patterns. The maximum likelihood training of the model follows an “analysis by synthesis” scheme and can be interpreted as a mode seeking and mode shifting process. The model can synthesize 3D shape patterns by sampling from the probability distribution via MCMC such as Langevin dynamics. The model can be used to train a 3D generator network via MCMC teaching. The conditional version of the 3D shape descriptor net can be used for 3D object recovery and 3D object super-resolution. Experiments demonstrate that the proposed model can generate realistic 3D shape patterns and can be useful for 3D shape analysis.},
  website={http://www.stat.ucla.edu/~jxie/3DDescriptorNet/3DDescriptorNet.html}
}

@inproceedings{xie2020generative,
  title={Generative VoxelNet: Learning Energy-Based Models for 3D Shape Synthesis and Analysis},
  author={Xie, Jianwen and Zheng, Zilong and Gao, Ruiqi and Wang, Wenguan and Zhu, Song-Chun and Wu, Ying Nian},
  equalauthor={Xie, Jianwen and Zheng, Zilong},
  booktitle={IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},
  year={2020},
  abbr={TPAMI},
  pdf={3DDescriptorNet/3DGConvNet_pami.pdf},
  abstract={3D data that contains rich geometry information of objects and scenes is a valuable asset for understanding 3D physical world. With the recent emergence of large-scale 3D datasets, it becomes increasingly crucial to have a powerful 3D generative model for 3D shape synthesis and analysis. This paper proposes a 3D shape descriptor network, which is a deep 3D convolutional energy-based model, for representing volumetric shape patterns. The maximum likelihood training of the model follows an “analysis by synthesis” scheme. The benefits of the proposed model are five-fold: first, unlike GANs and VAEs, the training of the model does not rely on any auxiliary models; second, the model can synthesize realistic 3D shapes by sampling from the probability distribution via MCMC, such as Langevin dynamics; third, the conditional version of the model can be applied to 3D object recovery and super-resolution; fourth, the model can be used to train a 3D generator network via MCMC teaching; fifth, the unsupervisedly trained model provides a powerful feature extractor for 3D data, which can be useful for 3D object classification. Experiments demonstrate that the proposed model can generate high-quality 3D shape patterns and can be useful for a wide variety of 3D shape analysis.},
  website={http://www.stat.ucla.edu/~jxie/3DEBM/}
}