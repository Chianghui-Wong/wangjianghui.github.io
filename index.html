<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Zilong&#39;s Blog">
    <meta name="author" content="Zilong Zheng">

    <title>Zilong Zheng | UCLA CS PhD</title>

    <!-- Bootstrap Core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Main CSS -->
    <link href="css/main.css" rel="stylesheet">

    <!-- Font Awesome CSS -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

    <!-- icon -->
    <link rel="shortcut icon" href="favicon.ico" type="image/x-icon">
    <link rel="icon" href="favicon.ico" type="image/x-icon">

    <!-- Google Fonts -->
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<!-- The #page-top ID is part of the scrolling feature - the data-spy and data-target are part of the built-in Bootstrap scrollspy function -->

<body id="page-top" data-spy="scroll" data-target=".navbar-fixed-top">

    <!-- Navigation -->
    <nav class="navbar navbar-default navbar-fixed-top navbar-inverse bg-inverse" role="navigation">
        <div class="container">
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand page-scroll active" href="#page-top">Zilong Zheng</a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse navbar-ex1-collapse">
                <ul class="nav navbar-nav">
                    <!-- Hidden li included to remove active class from about link when scrolled up past about section -->
                    <li class="hidden">
                        <a class="page-scroll" href="#page-top"></a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#about">About</a>
                    </li>
<!--                     <li>
                        <a class="page-scroll" href="#projects">Projects</a>
                    </li> -->
                    <li>
                        <a class="page-scroll" href="#publications">Publications</a>
                    </li>
<!--                     <li>
                        <a class="page-scroll" href="#awards">Awards</a>
                    </li>
-->                 <li>
                        <a href="files/zilong_cv.pdf">CV</a>
                    </li>
                 </ul>
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="mailto:zilongzheng0318@gmail.com" class="navbar-brand"><i class="fa fa-envelope"></i></a>
                    </li>
                    <li>
                        <a href="https://github.com/zilongzheng/" class="navbar-brand"><i class="fa fa-github"></i></a>
                    </li>
                    <li>
                        <a href="https://scholar.google.com/citations?user=9sDx70IAAAAJ&hl=en" class="navbar-brand"><i class="fa fa-graduation-cap"></i></a>
                    </li>
                    <li>
                        <a href="https://www.linkedin.com/in/zilongzheng0318" class="navbar-brand"><i class="fa fa-linkedin"></i></a>
                    </li>
                    <li>
                        <a href="https://www.facebook.com/zilongzheng0318" class="navbar-brand"><i class="fa fa-facebook"></i></a>
                    </li>
                    <li>
                        <a href="https://twitter.com/zilongzheng0318" class="navbar-brand"><i class="fa fa-twitter"></i></a>
                    </li>

                </ul>

            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

    <!-- Intro Section -->
    <section id="intro" class="intro-section">
        <div class="container">
            <div class="row">
                <div class="col-xs-12 col-sm-6 col-md-5 col-lg-4">
                        <img src="img/zlz.jpg" class="img-responsive img-circle img-fluid" width="200" />
                </div>
                <div class="col-xs-12 col-md-6 col-md-7 col-lg-8 personal-intro">
                    <div class="col-xs-12 col-sm-12 col-md-12">
                        <h2>Zilong Zheng</h2>
                        <h2><small>Ph.D. Candidate in Computer Science at UCLA</small></h2>
                        <address>
                            Engineering VI 491<br>
                            404 Westwood Plaza<br>
                            University of California, Los Angeles<br>
                            Los Angeles, CA, 90095
                        </address>
                        <span><strong>Email: </strong>z.zheng <code>[at]</code> ucla <code>[dot]</code> edu</span>
                    </div>
                </div>
            </div>
        </div>
    </section>
        <hr></hr>
    <!-- About Section -->
    <section id="about" class="about-section">
        <div class="container">
            <!-- <h1>About</h1> -->
            <div class="bio-info">
                <p>I am a fourth year Ph.D. candidate in the Department of Computer Science, UCLA. I am currently doing research on machine learning at <a href="http://vcla.stat.ucla.edu/">Center for Vision, Cognition, Learning, and Autonomy (VCLA)</a>, under the supervision of <a href="http://www.stat.ucla.edu/~sczhu/">Prof. Song-chun Zhu</a>. Before that, I obtained bachelor degree of Computer Science at University of Minnesota. I also received B.E. degree in Micro-electronic Technology from <a href="https://www.uestc.edu.cn/">University of Electronic Science and Technology of China (UESTC)</a>. My research interests lie in Machine Learning and Cognitive Science.</p>
            </div>
        </div>
    </section>
    <hr></hr>

    <!-- Projects Section -->
<!--     <section id="projects" class="projects-section">
        <div class="container">
            <div class="row">
                <div class="col-lg-12">
                    <h1>Projects</h1>
                </div>
            </div>
        </div>
    </section>
    <hr></hr> -->


    <section id="publications" class="publications-section">
        <div class="container">
            <div class="row">
                <div class="col-lg-12">
                    <h1>Publications</h1>
                    <!-- Preprints -->
                    <h2>Journal Articals</h2>
                    <div class="contents">
                        <ul class="list-group">

                            <!-- Cond CoopNets -->
                            <li class="list-group-item">
                                <div class="row">
                                    <div class="col-xs-12 col-sm-12 col-lg-12">
                                    <h4 class="list-group-item-heading"> Cooperative Training of Fast Thinking Initializer and Slow Thinking Solver for Multi-Modal Conditional Learning
                                    <span class="badge">
                                        <a class="conf" href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">TPAMI</a>
                                    </span></h4>
                                    <p class="detail">
                                        <a href="http://www.stat.ucla.edu/~jxie/">Jianwen Xie</a><sup>*</sup>, 
                                        <strong>Zilong Zheng</strong><sup>*</sup>, 
                                        <a href="https://fang-xiaolin.github.io/">Xiaolin Fang</a>, 
                                        <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>, 
                                        <a href="http://www.stat.ucla.edu/~ywu/">Ying Nian Wu</a> (* equal contributions)<br>
                                        <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>) (major revision)</em>
                                    </p>
                                    <button class="btn btn-success btn-xs" data-toggle="collapse" data-target='#condcoopnets-abs'>Abstract</button>
                                    <a class="btn btn-primary btn-xs" role="button" href="https://arxiv.org/pdf/1902.02812.pdf">PDF</a> 
                                    <!-- <a class="btn btn-info btn-xs" role="button" href="https://github.com/jianwen-xie/3DDescriptorNet">Code</a>  -->
                                    <!-- <a class="btn btn-warning btn-xs" role="button" href="http://www.stat.ucla.edu/~jxie/3DDescriptorNet/3DDescriptorNet.html">Website</a>  -->
                                    <!-- <button class="btn btn-bib btn-xs" data-toggle="collapse" data-target='#3ddescriptor-bib'>BibTex</button> -->
                                    </div>
                                </div>
                                <div id="condcoopnets-abs" class="collapse abstract">
                                    This paper studies the supervised learning of the conditional distribution of a high-dimensional output given an input, where
                                    the output and input may belong to two different modalities, e.g., the output is an photo image and the input is a sketch image. We solve
                                    this problem by cooperative training of a fast thinking initializer and slow thinking solver. The initializer generates the output directly by a
                                    non-linear transformation of the input as well as a noise vector that accounts for latent variability in the output. The slow thinking solver
                                    learns an objective function in the form of a conditional energy function, so that the output can be generated by optimizing the objective
                                    function, or more rigorously by sampling from the conditional energy-based model. We propose to learn the two models jointly, where the
                                    fast thinking initializer serves to initialize the sampling of the slow thinking solver, and the solver refines the initial output by an iterative
                                    algorithm. The solver learns from the difference between the refined output and the observed output, while the initializer learns from how
                                    the solver refines its initial output. We demonstrate the effectiveness of the proposed method on various multi-modal conditional learning
                                    tasks, e.g., class-to-image generation, image-to-image translation, and image recovery.                               
                                </div>
                                <div id="condcoopnets-bib" class="collapse abstract">
                                    <pre>
@inproceedings{xie20183DDesNet,
title={Learning Descriptor Networks for 3D Shape Synthesis and Analysis},
author={Xie, Jianwen and Zheng, Zilong and Gao, Ruiqi and Wang, Wenguan and Zhu Song-Chun and Wu, Ying Nian},
booktitle={The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
year={2018}
}                                        </pre>
                                </div>
                            </li>

                            <!-- 3D Descriptor PAMI-->
                            <li class="list-group-item">
                                <div class="row">
                                    <div class="col-xs-12 col-sm-12 col-lg-12">
                                    <h4 class="list-group-item-heading"> Generative VoxelNet: Learning Energy-Based Models for 3D Shape Synthesis and Analysis
                                    <span class="badge">
                                        <a class="conf" href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">TPAMI</a>
                                    </span></h4>
                                    <p class="detail">
                                        <a href="http://www.stat.ucla.edu/~jxie/">Jianwen Xie</a><sup>*</sup>, 
                                        <strong>Zilong Zheng</strong><sup>*</sup>, 
                                        <a href="http://www.stat.ucla.edu/~ruiqigao/">Ruiqi Gao</a>, 
                                        <a href="https://sites.google.com/view/wenguanwang">Wenguan Wang</a>, 
                                        <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>, 
                                        <a href="http://www.stat.ucla.edu/~ywu/">Ying Nian Wu</a> (* equal contributions)<br>
                                        <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>)</em>
                                    </p>
                                    <button class="btn btn-success btn-xs" data-toggle="collapse" data-target='#3dgvoxelnet-abs'>Abstract</button>
                                    <a class="btn btn-primary btn-xs" role="button" href="./projects/3DDescriptorNet/3DGConvNet_pami.pdf">PDF</a> 
                                    <!-- <a class="btn btn-info btn-xs" role="button" href="https://github.com/jianwen-xie/3DDescriptorNet">Code</a>  -->
                                    <!-- <a class="btn btn-warning btn-xs" role="button" href="http://www.stat.ucla.edu/~jxie/3DDescriptorNet/3DDescriptorNet.html">Website</a>  -->
                                    <button class="btn btn-bib btn-xs" data-toggle="collapse" data-target='#3dgvoxelnet-bib'>BibTex</button>
                                    </div>
                                </div>
                                <div id="3dgvoxelnet-abs" class="collapse abstract">
                                    3D data that contains rich geometry information of objects and scenes is a valuable asset for understanding 3D physical world.
                                    With the recent emergence of large-scale 3D datasets, it becomes increasingly crucial to have a powerful 3D generative model for 3D
                                    shape synthesis and analysis. This paper proposes a 3D shape descriptor network, which is a deep 3D convolutional energy-based
                                    model, for representing volumetric shape patterns. The maximum likelihood training of the model follows an “analysis by synthesis”
                                    scheme. The benefits of the proposed model are five-fold: first, unlike GANs and VAEs, the training of the model does not rely on any
                                    auxiliary models; second, the model can synthesize realistic 3D shapes by sampling from the probability distribution via MCMC, such as
                                    Langevin dynamics; third, the conditional version of the model can be applied to 3D object recovery and super-resolution; fourth, the
                                    model can be used to train a 3D generator network via MCMC teaching; fifth, the unsupervisedly trained model provides a powerful
                                    feature extractor for 3D data, which can be useful for 3D object classification. Experiments demonstrate that the proposed model can
                                    generate high-quality 3D shape patterns and can be useful for a wide variety of 3D shape analysis.                                
                                </div>
                                <div id="3dgvoxelnet-bib" class="collapse abstract">
                                    <pre>
@inproceedings{xie2020generative,
    title={Generative VoxelNet: Learning Energy-Based Models for 3D Shape Synthesis and Analysis},
    author={Xie, Jianwen and Zheng, Zilong and Gao, Ruiqi and Wang, Wenguan and Zhu Song-Chun and Wu, Ying Nian},
    booktitle={IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},
    year={2020}
}                                        </pre>
                                </div>
                            </li>
                            
                        </ul>
                    </div>

                    <!-- Conference -->
                    <h2>Conference Papers</h2>
                        <div class="contents">
                            <ul class="list-group">
                                <!-- CycleCoop -->
                                <li class="list-group-item">
                                    <div class="row">
                                        <div class="col-xs-12 col-sm-4 col-lg-2">
                                            <img src="projects/CycleCoop/style_transfer.png" class="img-responsive img-fluid"/>
                                        </div>
                                        <div class="col-xs-12 col-sm-8 col-lg-10">
                                        <h4 class="list-group-item-heading"> Unsupervised Cross-Domain Translation via Alternating MCMC Teaching
                                        </h4>
                                        <p class="detail">
                                            <a href="http://www.stat.ucla.edu/~jxie/">Jianwen Xie</a><sup>*</sup>, 
                                            <strong>Zilong Zheng</strong><sup>*</sup>, 
                                            <a href="https://sites.google.com/view/wenguanwang">Xiaolin Fang</a>, 
                                            <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>, 
                                            <a href="http://www.stat.ucla.edu/~ywu/">Ying Nian Wu</a> (* equal contributions)<br>
                                            <em>Under Review</em>
                                        </p>                                
                                    </div>
                                </li>


                                <!-- GPointNet -->
                                <li class="list-group-item">
                                    <div class="row">
                                        <div class="col-xs-12 col-sm-4 col-lg-2">
                                            <img src="projects/GPointNet/gen_res.png" class="img-responsive img-fluid"/>
                                        </div>
                                        <div class="col-xs-12 col-sm-8 col-lg-10">
                                        <h4 class="list-group-item-heading"> Generative PointNet: Deep Energy-Based Learning on Unordered Point Sets for 3D Generation, Reconstruction and Classification
                                        
                                        </h4>
                                        <p class="detail">
                                            <a href="http://www.stat.ucla.edu/~jxie/">Jianwen Xie</a>,
                                            <a href="http://www.stat.ucla.edu/~jxie/">Yifei Xu</a>,
                                            <strong>Zilong Zheng</strong>, 
                                            <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>, 
                                            <a href="http://www.stat.ucla.edu/~ywu/">Ying Nian Wu</a> <br>
                                            <em>Under Review</em>
                                        </p>                                
                                    </div>
                                </li>


                                
                                <!-- TomICRA20 -->
                                <li class="list-group-item">
                                    <div class="row">
                                        <div class="col-xs-12 col-sm-4 col-lg-2">
                                            <img src="projects/TomICRA20/tom_icra20.png" class="img-responsive img-fluid"/>
                                        </div>
                                        <div class="col-xs-12 col-sm-8 col-lg-10">
                                        <h4 class="list-group-item-heading"> Joint Inference of States, Robot Knowledge, and Human (False-)Beliefs
                                        
                                            <a class="conf" href="https://www.icra2020.org/"><span class="badge">ICRA'20</span></a>
                                        </h4>
                                        <p class="detail">
                                            <a href="https://ytyt.yt/">Tao Yuan</a>,
                                            <a href="https://liuhx111.github.io/">Hangxin Liu</a>,  
                                            <a href="https://lifengfan.github.io/">Lifeng Fan</a>,  
                                            <strong>Zilong Zheng</strong>, 
                                            <a href="http://www.stat.ucla.edu/~taogao/">Tao Gao</a>,  
                                            <a href="https://yzhu.io/">Yixin Zhu</a>,  
                                            <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a> <br>
                                            <em>In Proceedings of the IEEE International Conference on Robotics and Automation (<strong>ICRA</strong>)</em> 2020 <span></span>
                                        </p>
                                        <button class="btn btn-success btn-xs" data-toggle="collapse" data-target='#yuan2020joint-abs'>Abstract</button>
                                        <a class="btn btn-primary btn-xs" role="button" href="projects/TomICRA20/icra20_tom.pdf">PDF</a> 
                                        <a class="btn btn-danger btn-xs" role="button" href="https://vimeo.com/391734593">Video</a> 
                                        <!-- <a class="btn btn-info btn-xs" role="button" href="https://github.com/zilongzheng/visdial-gnn">Code</a>  -->
                                        <button class="btn btn-bib btn-xs" data-toggle="collapse" data-target='#yuan2020joint-bib'>BibTex</button>
                                        </div>
                                    </div>
                                    <div id="yuan2020joint-abs" class="collapse abstract">
                                        Aiming to understand how human (false-)belief—a core socio-cognitive ability—would affect human interactions with robots, this paper proposes to adopt a graphical model to unify the representation of object states, robot knowledge, and human (false-)beliefs. Specifically, a parse graph (PG) is learned from a single-view spatiotemporal parsing by aggregating various object states along the time; such a learned representation is accumulated as the robot’s knowledge. An inference algorithm is derived to fuse individual PG from all robots across multi-views into a joint PG, which affords more effective reasoning and inference capability to overcome the errors originated from a single view. In the experiments, through the joint inference over PGs, the system correctly recognizes human (false-)belief in various settings and achieves better cross-view accuracy on a challenging small object tracking dataset.                                    </div>
                                    <div id="yuan2020joint-bib" class="collapse abstract">
<pre>
@inproceedings{yuan2020joint,
    title={Joint Inference of States, Robot Knowledge, and Human (False-)Beliefs},
    author={Yuan, Tao and Liu, Hangxin and Fan, Lifeng and Zheng, Zilong and Gao, Tao and Zhu, Yixin and Zhu, Song-Chun},
    booktitle={ICRA},
    year={2020}
}
</pre>
                                    </div>
                                </li>


                                <!-- Motion-DynGen -->
                                <li class="list-group-item">
                                    <div class="row">
                                        <div class="col-xs-12 col-sm-4 col-lg-2">
                                            <img src="projects/Motion-DynGen/deform.gif" class="img-responsive img-fluid"/>
                                        </div>
                                        <div class="col-xs-12 col-sm-8 col-lg-10">
                                        <h4 class="list-group-item-heading"> Motion-Based Generator Model: Unsupervised Disentanglement of Appearance, Trackable and Intrackable Motions in Dynamic Patterns
                                        
                                            <a class="conf" href="https://aaai.org/Conferences/AAAI-20/"><span class="badge">AAAI'20</span></a>
                                        </h4>
                                        <p class="detail">
                                            <a href="http://www.stat.ucla.edu/~jxie/">Jianwen Xie</a><sup>*</sup>,
                                            <a href="http://www.stat.ucla.edu/~ruiqigao/">Ruiqi Gao</a><sup>*</sup>,  
                                            <strong>Zilong Zheng</strong>, 
                                            <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>, 
                                            <a href="http://www.stat.ucla.edu/~ywu/">Ying Nian Wu</a> (* equal contributions)<br>
                                            <em>The Thirty-Third AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>)</em> 2020 <span>(Oral)</span>
                                        </p>
                                        <button class="btn btn-success btn-xs" data-toggle="collapse" data-target='#motion-dygen-abs'>Abstract</button>
                                        <a class="btn btn-primary btn-xs" role="button" href="https://arxiv.org/pdf/1911.11294.pdf">PDF</a> 
                                        <a class="btn btn-warning btn-xs" role="button" href="http://www.stat.ucla.edu/~jxie/MotionBasedGenerator/MotionBasedGenerator.html">Website</a> 
                                        <!-- <a class="btn btn-info btn-xs" role="button" href="https://github.com/zilongzheng/visdial-gnn">Code</a>  -->
                                        <button class="btn btn-bib btn-xs" data-toggle="collapse" data-target='#motion-dygen-bib'>BibTex</button>
                                        </div>
                                    </div>
                                    <div id="motion-dygen-abs" class="collapse abstract">
                                        Dynamic patterns are characterized by complex spatial and motion patterns. Understanding dynamic patterns requires a disentangled representational model that separates the factorial components. A commonly used model for dynamic patterns is the state space model, where the state evolves over time according to a transition model and the state generates the observed image frames according to an emission model. To model the motions explicitly, it is natural for the model to be based on the motions or the displacement fields of the pixels. Thus in the emission model, we let the hidden state generate the displacement field, which warps the trackable component in the previous image frame to generate the next frame while adding a simultaneously emitted residual image to account for the change that cannot be explained by the deformation. The warping of the previous image is about the trackable part of the change of image frame, while the residual image is about the intrackable part of the image. We use a maximum likelihood algorithm to learn the model parameters that iterates between inferring latent noise vectors that drive the transition model and updating the parameters given the inferred latent vectors. Meanwhile we adopt a regularization term to penalize the norms of the residual images to encourage the model to explain the change of image frames by trackable motion. Unlike existing methods on dynamic patterns, we learn our model in unsupervised setting without ground truth displacement fields or optical flows. In addition, our model defines a notion of intrackability by the separation of warped component and residual component in each image frame. We show that our method can synthesize realistic dynamic pattern, and disentangling appearance, trackable and intrackable motions. The learned models can be useful for motion transfer, and it is natural to adopt it to define and measure intrackability of a dynamic pattern.                               
                                    </div>
                                    <div id="motion-dygen-bib" class="collapse abstract">
                                        <pre>
@inproceedings{xie2020motion,
    title={Motion-Based Generator Model: Unsupervised Disentanglement of Appearance, Trackable and Intrackable Motions in Dynamic Patterns},
    author={Xie, Jianwen and Gao, Ruiqi and Zheng, Zilong and Zhu, Song-Chun and Wu, Ying Nian},
    booktitle={The Thirty-Third AAAI Conference on Artificial Intelligence (AAAI)},
    year={2020}
} 
</pre>
                                    </div>
                                </li>

                                <!-- VisDial-GNN -->
                                <li class="list-group-item">
                                    <div class="row">
                                        <div class="col-xs-12 col-sm-4 col-lg-2">
                                            <img src="projects/VisDial-GNN/visdial.jpg" class="img-responsive img-fluid"/>
                                        </div>
                                        <div class="col-xs-12 col-sm-8 col-lg-10">
                                        <h4 class="list-group-item-heading"> Reasoning Visual Dialogs with Structural and Partial Observations
                                        
                                            <a class="conf" href="http://cvpr2019.thecvf.com/"><span class="badge">CVPR'19</span></a>
                                        </h4>
                                        <p class="detail">
                                            <strong>Zilong Zheng</strong><sup>*</sup>, 
                                            <a href="https://sites.google.com/view/wenguanwang">Wenguan Wang</a><sup>*</sup>,
                                            <a href="https://web.cs.ucla.edu/~syqi/">Siyuan Qi</a><sup>*</sup>,  
                                            <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a> (* equal contributions)<br>
                                            <em>IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em> 2019 <span>(Oral)</span>
                                        </p>
                                        <button class="btn btn-success btn-xs" data-toggle="collapse" data-target='#visdial-gnn-abs'>Abstract</button>
                                        <a class="btn btn-primary btn-xs" role="button" href="https://arxiv.org/pdf/1904.05548.pdf">PDF</a> 
                                        <a class="btn btn-info btn-xs" role="button" href="https://github.com/zilongzheng/visdial-gnn">Code</a> 
                                        <button class="btn btn-bib btn-xs" data-toggle="collapse" data-target='#visdial-gnn-bib'>BibTex</button>
                                        </div>
                                    </div>
                                    <div id="visdial-gnn-abs" class="collapse abstract">
                                        We propose a novel model to address the task of Visual
                                        Dialog which exhibits complex dialog structures. To obtain
                                        a reasonable answer based on the current question and the
                                        dialog history, the underlying semantic dependencies between
                                        dialog entities are essential. In this paper, we explicitly
                                        formalize this task as inference in a graphical model
                                        with partially observed nodes and unknown graph structures
                                        (relations in dialog). The given dialog entities are
                                        viewed as the observed nodes. The answer to a given question
                                        is represented by a node with missing value. We first
                                        introduce an Expectation Maximization algorithm to infer
                                        both the underlying dialog structures and the missing node
                                        values (desired answers). Based on this, we proceed to propose
                                        a differentiable graph neural network (GNN) solution
                                        that approximates this process. Experiment results on the
                                        VisDial and VisDial-Q datasets show that our model outperforms
                                        comparative methods. It is also observed that our
                                        method can infer the underlying dialog structure for better
                                        dialog reasoning.                                    
                                    </div>
                                    <div id="visdial-gnn-bib" class="collapse abstract">
                                        <pre>
@inproceedings{zheng2019reasoning,
    title={Reasoning Visual Dialogs with Structural and Partial Observations},
    author={Zheng, Zilong and Wang, Wenguan and Qi, Siyuan and Zhu, Song-Chun},
    booktitle={Computer Vision and Pattern Recognition (CVPR), 2019 IEEE Conference on},
    year={2019}
} 
</pre>
                                    </div>
                                </li>


                                <!-- Dynamic Generator -->
                                <li class="list-group-item">
                                    <div class="row">
                                        <div class="col-xs-12 col-sm-4 col-lg-2">
                                            <img src="projects/DynamicGenerator/dynamic_generator.gif" class="img-responsive img-fluid"/>
                                        </div>
                                        <div class="col-xs-12 col-sm-8 col-lg-10">
                                        <h4 class="list-group-item-heading">Learning Dynamic Generator Model by Alternating Back-Propagation Through Time 
                                        
                                            <a class="conf" href="https://aaai.org/Conferences/AAAI-19/"><span class="badge">AAAI'19</span></a>
                                        </h4>
                                        <p class="detail">
                                            <a href="http://www.stat.ucla.edu/~jxie/">Jianwen Xie</a><sup>*</sup>,
                                            <a href="http://www.stat.ucla.edu/~ruiqigao/">Ruiqi Gao</a><sup>*</sup>,  
                                            <strong>Zilong Zheng</strong>, 
                                            <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>, 
                                            <a href="http://www.stat.ucla.edu/~ywu/">Ying Nian Wu</a> (* equal contributions)<br>
                                            <em>The Thirty-Third AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>)</em> 2019 <span>(Spotlight)</span>
                                        </p>
                                        <button class="btn btn-success btn-xs" data-toggle="collapse" data-target='#dynamic_generator-abs'>Abstract</button>
                                        <a class="btn btn-primary btn-xs" role="button" href="https://arxiv.org/pdf/1812.10587.pdf">PDF</a> 
                                        <a class="btn btn-info btn-xs" role="button" href="https://github.com/jianwen-xie/Dynamic_generator">Code</a> 
                                        <a class="btn btn-warning btn-xs" role="button" href="http://www.stat.ucla.edu/~jxie/DynamicGenerator/DynamicGenerator.html">Website</a> 
                                        <button class="btn btn-bib btn-xs" data-toggle="collapse" data-target='#dynamic_generator-bib'>BibTex</button>
                                        </div>
                                    </div>
                                    <div id="dynamic_generator-abs" class="collapse abstract">
                                        This paper studies the dynamic generator model for spatial-temporal processes such as dynamic textures and action sequences in video data. In this model, each time frame of the video sequence is generated by a generator model, which is a non-linear transformation of a latent state vector, where the non-linear transformation is parametrized by a top-down neural network. The sequence of latent state vectors follows a non-linear auto-regressive model, where the state vector of the next frame is a non-linear transformation of the state vector of the current frame as well as an independent noise vector that provides randomness in the transition. The non-linear transformation of this transition model can be parametrized by a feedforward neural network. We show that this model can be learned by an alternating back-propagation through time algorithm that iteratively samples the noise vectors and updates the parameters in the transition model and the generator model. We show that our training method can learn realistic models for dynamic textures and action patterns.
                                    </div>
                                    <div id="dynamic_generator-bib" class="collapse abstract">
                                        <pre>
@article{xie2019DG,
    title = {Learning Dynamic Generator Model by Alternating Back-Propagation Through Time},
    author = {Xie, Jianwen and Gao, Ruiqi and Zheng, Zilong and Zhu, Song-Chun and Wu, Ying Nian},
    journal={The Thirty-Third AAAI Conference on Artificial Intelligence (AAAI)},
    year = {2019}
}                                        </pre>
                                    </div>
                                </li>

                                <!-- 3D Descriptor -->
                                <li class="list-group-item">
                                    <div class="row">
                                        <div class="col-xs-12 col-sm-4 col-lg-2">
                                            <img src="projects/3DDescriptorNet/3ddescriptor.png" class="img-responsive img-fluid"/>
                                        </div>
                                        <div class="col-xs-12 col-sm-8 col-lg-10">
                                        <h4 class="list-group-item-heading">Learning Descriptor Networks for 3D Shape Synthesis and Analysis 
                                        <span class="badge">
                                            <a class="conf" href="http://cvpr2018.thecvf.com/">CVPR'18</a>
                                        </span></h4>
                                        <p class="detail">
                                            <a href="http://www.stat.ucla.edu/~jxie/">Jianwen Xie</a><sup>*</sup>, 
                                            <strong>Zilong Zheng</strong><sup>*</sup>, 
                                            <a href="http://www.stat.ucla.edu/~ruiqigao/">Ruiqi Gao</a>, 
                                            <a href="https://sites.google.com/view/wenguanwang">Wenguan Wang</a>, 
                                            <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>, 
                                            <a href="http://www.stat.ucla.edu/~ywu/">Ying Nian Wu</a> (* equal contributions)<br>
                                            <em>IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em> 2018 <span>(Oral)</span>
                                        </p>
                                        <button class="btn btn-success btn-xs" data-toggle="collapse" data-target='#3ddescriptor-abs'>Abstract</button>
                                        <a class="btn btn-primary btn-xs" role="button" href="https://arxiv.org/pdf/1804.00586.pdf">PDF</a> 
                                        <a class="btn btn-info btn-xs" role="button" href="https://github.com/jianwen-xie/3DDescriptorNet">Code</a> 
                                        <a class="btn btn-warning btn-xs" role="button" href="http://www.stat.ucla.edu/~jxie/3DDescriptorNet/3DDescriptorNet.html">Website</a> 
                                        <button class="btn btn-bib btn-xs" data-toggle="collapse" data-target='#3ddescriptor-bib'>BibTex</button>
                                        </div>
                                    </div>
                                    <div id="3ddescriptor-abs" class="collapse abstract">
                                        This paper proposes a 3D shape descriptor network, which is a deep convolutional energy-based model, for modeling volumetric shape patterns. The maximum likelihood training of the model follows an “analysis by synthesis” scheme and can be interpreted as a mode seeking and mode shifting process. The model can synthesize 3D shape patterns by sampling from the probability distribution via MCMC such as Langevin dynamics. The model can be used to train a 3D generator network via MCMC teaching. The conditional version of the 3D shape descriptor net can be used for 3D object recovery and 3D object super-resolution. Experiments demonstrate that the proposed model can generate realistic 3D shape patterns and can be useful for 3D shape analysis.
                                    </div>
                                    <div id="3ddescriptor-bib" class="collapse abstract">
                                        <pre>
@inproceedings{xie20183DDesNet,
    title={Learning Descriptor Networks for 3D Shape Synthesis and Analysis},
    author={Xie, Jianwen and Zheng, Zilong and Gao, Ruiqi and Wang, Wenguan and Zhu Song-Chun and Wu, Ying Nian},
    booktitle={The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    year={2018}
}                                        </pre>
                                    </div>
                                </li>
                            </ul>
                        </div>
                </div>
            </div>
        </div>
    </section>
    <hr></hr>

    <section class="patents-section">
        <div class="container">
            <div class="row">
                <div class="col-md-12">
                    <h1>Patents</h1>
                    <div class="contents">
                        <ul class="list-group">
                        <li class="list-group-item">
                            <span><strong>Noninvasive brain blood oxygen parameter measuring method</strong></span>
                            <p class="detail">
                                Patent Publication No.: <a href="https://patents.google.com/patent/CN104382604A/en">CN104382604A</a>, Priority-date: 2014-12-02. 
                                <a class="btn btn-primary btn-xs" role="button" href="https://patentimages.storage.googleapis.com/d8/d8/f6/efcc7576ae1986/CN104382604A.pdf">PDF</a> 
                            </p>
                        </li>
                        <li class="list-group-item">
                            <span><strong>Near infrared noninvasive detection probe for tissue blood oxygen saturation</strong></span>
                            <p class="detail">
                                Patent Publication No.: <a href="https://patents.google.com/patent/CN204394526U/en">CN204394526U</a>, Priority-date: 2014-12-02. <a class="btn btn-primary btn-xs" role="button" href="https://patentimages.storage.googleapis.com/b7/b8/89/cdf3ab887e6bc9/CN204394526U.pdf">PDF</a> 
                            </p>
                        </li>
                        <li class="list-group-item">
                            <span><strong>Brain blood oxygen saturation degree noninvasive monitor</strong></span>
                            <p class="detail">
                                Patent Publication No.: <a href="https://patents.google.com/patent/CN204394527U/en">CN204394527U</a>, Priority-date: 2014-12-02. <a class="btn btn-primary btn-xs" role="button" href="https://patentimages.storage.googleapis.com/0a/d6/02/15ac8e6e31995e/CN204394527U.pdf">PDF</a> 
                            </p>
                        </li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
    </section>
            <hr></hr>
    <!-- Awards Section -->
<!--     <section id="awards" class="awards-section">
        <div class="container">
            <div class="row">
                <div class="col-md-12">
                    <h1>Honors & Awards</h1>
                    <div class="row contents">
                        <table class="table table-striped">
                            <tbody>
                                <tr>
                                    <td>Excellent College Graduate of Sichuan Province</td><td>2016</td>
                                </tr>
                                <tr>
                                    <td>UESTC Outstanding Graduate Award (10/4990)</td><td>2016</td>
                                </tr>
                                <tr>
                                    <td>Top Prize of People Scholarship</td><td>2014</td>
                                </tr>
                                <tr>
                                    <td>National Scholarship</td><td>2013</td>
                                </tr>
                                <tr>
                                    <td>The First Prize of China High School Chemistry Olympiad (Jiangsu Province)</td><td>2011</td>
                                </tr>
                            </tbody>
                        </table>
                    </div>
                </div>
            </div>
        </div>
    </section>
 -->    

    <section id="professional-service" class="other-section">
        <div class="container">
            <div class="row">
                <div class="col-md-12">
                    <h1>Professional Services</h1>
                    <div class="contents">
                        <ul>
                            <li>Conference reviewer for ECCV 2020; CVPR 2019, 2020; AAAI 2020; ICCV 2019</li>
                            <li>Journal reviewer for Pattern Recognition, Neurocomputing</li>
                        </ul>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <footer class="footer">
        <div class="container">
            <p class="text-muted">&copy; 2020 by Zilong Zheng. <span class="hidden-phone">All rights reserved</span></p>
        </div>
    </footer>

    <!-- jQuery -->
    <script src="js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

    <!-- Scrolling Nav JavaScript -->
    <script src="js/jquery.easing.min.js"></script>
    <script src="js/scrolling-nav.js"></script>

</body>

</html>
